network init start

hyperparameters
init_weight_range 0.000000 ,xavier
learning_rate 0.000050
lambda1 0.000000
lambda2 0.000000
dropout 0.400000
minibatch size 32

   DENSE CONVOLUTION  [  416   272     3] [    3     3     8] [  416   272    11] [       216          8][  25346048]
                RELU  [  416   272    11] [    1     1     1] [  416   272    11] [         0          0][   2489344]
   DENSE CONVOLUTION  [  416   272    11] [    3     3     8] [  416   272    19] [       792          8][  90521600]
                RELU  [  416   272    19] [    1     1     1] [  416   272    19] [         0          0][   4299776]
   DENSE CONVOLUTION  [  416   272    19] [    3     3     8] [  416   272    27] [      1368          8][ 155697152]
                RELU  [  416   272    27] [    1     1     1] [  416   272    27] [         0          0][   6110208]
   DENSE CONVOLUTION  [  416   272    27] [    3     3     8] [  416   272    35] [      1944          8][ 220872704]
                RELU  [  416   272    35] [    1     1     1] [  416   272    35] [         0          0][   7920640]
         CONVOLUTION  [  416   272    35] [    1     1    16] [  416   272    16] [       560         16][  65175552]
                RELU  [  416   272    16] [    1     1     1] [  416   272    16] [         0          0][   3620864]
         MAX POOLING  [  416   272    16] [    2     2     1] [  208   136    16] [         0          0][   3620864]
   DENSE CONVOLUTION  [  208   136    16] [    3     3     8] [  208   136    24] [      1152          8][  32814080]
                RELU  [  208   136    24] [    1     1     1] [  208   136    24] [         0          0][   1357824]
   DENSE CONVOLUTION  [  208   136    24] [    3     3     8] [  208   136    32] [      1728          8][  49107968]
                RELU  [  208   136    32] [    1     1     1] [  208   136    32] [         0          0][   1810432]
   DENSE CONVOLUTION  [  208   136    32] [    3     3     8] [  208   136    40] [      2304          8][  65401856]
                RELU  [  208   136    40] [    1     1     1] [  208   136    40] [         0          0][   2263040]
   DENSE CONVOLUTION  [  208   136    40] [    3     3     8] [  208   136    48] [      2880          8][  81695744]
                RELU  [  208   136    48] [    1     1     1] [  208   136    48] [         0          0][   2715648]
         CONVOLUTION  [  208   136    48] [    1     1    32] [  208   136    32] [      1536         32][  44355584]
                RELU  [  208   136    32] [    1     1     1] [  208   136    32] [         0          0][   1810432]
         MAX POOLING  [  208   136    32] [    2     2     1] [  104    68    32] [         0          0][   1810432]
         CONVOLUTION  [  104    68    32] [    3     3    32] [  104    68    32] [      9216         32][  65401856]
             DROPOUT  [  104    68    32] [    1     1     1] [  104    68    32] [         0          0][    452608]
         CONVOLUTION  [  104    68    32] [    8     8     3] [  104    68     3] [      6144          3][  43471584]

input_geometry  [416 272 3]
output_geometry [104 68 3]
network flops operations 980143872 FLOPS, 0.980144 GFLOPS
init DONE

network destructor done
